{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Exploring Generative Capabilities of Diffusion-Based Deep Generative Models <br><br> COMP3547 Deep Learning Assignment 2022/2023</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work based on several sources. Below are the most important attributions:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [2020, Denoising Diffusion Probabilistic Models (Ho, Jain, Abbeel)](https://arxiv.org/pdf/2006.11239.pdf)\n",
    "\n",
    "* [2021, Improved Denoising Diffusion Probabilistic Models (Nichol, Dhariwal)](https://proceedings.mlr.press/v139/nichol21a/nichol21a.pdf)\n",
    "\n",
    "* [2021, Diffusion Models Beat GANs on Image Synthesis (Dhariwal, Nichol)](https://arxiv.org/pdf/2105.05233v4.pdf)\n",
    "\n",
    "<br>\n",
    "\n",
    "* [2019, Generative Modeling by Estimating Gradients of the Data Distribution (Song, Ermon)](https://proceedings.neurips.cc/paper/2019/file/3001ef257407d5a371a96dcd947c7d93-Paper.pdf)\n",
    "\n",
    "* [2020, Score-based Generative Modeling Through Stochastic Differential Equations (Song, Sohl-Dickstein, Kingma, Kumar, Ermon, Poole)](https://arxiv.org/pdf/2011.13456.pdf)\n",
    "\n",
    "* [2020, Improved Techniques for Training Score-based Generative Models (Song, Ermon)](https://proceedings.neurips.cc/paper/2020/file/92c3b916311a5517d9290576e3ea37ad-Paper.pdf)\n",
    "\n",
    "* [2020, Denoising Diffusion Implicit Models (Song, Meng, Ermon)](https://arxiv.org/pdf/2010.02502.pdf)\n",
    "\n",
    "\n",
    "* [2022, High-Resolution Image Synthesis with (Rombach, Blattmann, Lorenz, Esser, Ommer)](https://arxiv.org/pdf/2112.10752.pdf)\n",
    "\n",
    "* [2022, Diffusion Models: A Comprehensive Survey of Methods and Applications (Yang et al.)](https://arxiv.org/abs/2209.00796)\n",
    "\n",
    "\n",
    "* [2022, How Much is Enough? A Study on Diffusion Times in Score-based Generative Models (Franzese et al.)](https://arxiv.org/abs/2206.05173)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* [https://github.com/dome272/Diffusion-Models-pytorch (Apache License 2.0)](https://github.com/dome272/Diffusion-Models-pytorch)\n",
    "\n",
    "* [https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing (MIT License)](https://colab.research.google.com/drive/1sjy9odlSSy0RBVgMTgP7s99NXsqglsUL?usp=sharing)\n",
    "\n",
    "* [https://github.com/lucidrains/denoising-diffusion-pytorch (MIT License)](https://github.com/lucidrains/denoising-diffusion-pytorch)\n",
    "\n",
    "* [https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=3a159023 (No license information)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/annotated_diffusion.ipynb#scrollTo=3a159023)\n",
    "\n",
    "* [https://github.com/labmlai (MIT License)](https://github.com/labmlai)\n",
    "\n",
    "* [https://github.com/heejkoo/Awesome-Diffusion-Models (MIT License)](https://github.com/heejkoo/Awesome-Diffusion-Models)\n",
    "\n",
    "* [https://github.com/yang-song/score_sde (Apache License 2.0)](https://github.com/yang-song/score_sde)\n",
    "\n",
    "* [https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing#scrollTo=XCR6m0HjWGVV (Apache License 2.0)](https://colab.research.google.com/drive/120kYYBOVa1i0TD85RjlEkFjaWDxSFUx3?usp=sharing#scrollTo=XCR6m0HjWGVV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import training\n",
    "import sampling\n",
    "import dataset\n",
    "import config\n",
    "\n",
    "import tqdm\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(config.SEED)\n",
    "torch.cuda.manual_seed_all(config.SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_loader = dataset.get_data(config.DATASET_NAME, config.BATCH_SIZE)\n",
    "\n",
    "model = model.ScoreMatchingModel(\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    channels=config.CHANNELS,\n",
    "    image_size=config.IMAGE_SIZE,\n",
    "    dimensions=config.DIMENSIONS,\n",
    "    embedding_size=config.EMBEDDING_SIZE,\n",
    "    groups_number=config.GROUPS_NUMBER,\n",
    "    epsilon=config.EPSILON,\n",
    "    sigma=config.SIGMA,\n",
    "    scale=config.SCALE,\n",
    "    T=config.T,\n",
    "    device=config.DEVICE,\n",
    ")\n",
    "\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1/10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:   0%|          | 0/781 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Placeholder storage has not been allocated on MPS device!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 44\u001b[0m\n\u001b[1;32m     41\u001b[0m x_noised \u001b[39m=\u001b[39m x \u001b[39m+\u001b[39m z \u001b[39m*\u001b[39m standard_deviation[:, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m     43\u001b[0m \u001b[39m# Calculate score\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m score \u001b[39m=\u001b[39m model(x_noised, t)\n\u001b[1;32m     46\u001b[0m \u001b[39m# Calculate loss\u001b[39;00m\n\u001b[1;32m     47\u001b[0m loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(torch\u001b[39m.\u001b[39msum((score \u001b[39m*\u001b[39m standard_deviation[:, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m] \u001b[39m+\u001b[39m z)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m, dim\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m3\u001b[39m)))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Dropbox/_COURSEWORK_GITHUB/DL/_github/score-matching/model.py:207\u001b[0m, in \u001b[0;36mScoreMatchingModel.forward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    202\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    203\u001b[0m     x,\n\u001b[1;32m    204\u001b[0m     t\n\u001b[1;32m    205\u001b[0m ):\n\u001b[1;32m    206\u001b[0m     \u001b[39m# Feature embedding for t\u001b[39;00m\n\u001b[0;32m--> 207\u001b[0m     feature_embedding \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mswish_activation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membed(t))\n\u001b[1;32m    209\u001b[0m     \u001b[39m# ENCODING\u001b[39;00m\n\u001b[1;32m    210\u001b[0m     x_1 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvolution_1(x)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/deep-learning/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Placeholder storage has not been allocated on MPS device!"
     ]
    }
   ],
   "source": [
    "if config.CHECKPOINT_FILE is not None:\n",
    "    checkpoint = torch.load(config.CHECKPOINT_FILE)\n",
    "    epoch_start = checkpoint[\"epoch\"]\n",
    "    loss = checkpoint[\"loss\"]\n",
    "    losses = checkpoint[\"losses\"]\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimiser.load_state_dict(checkpoint[\"optimiser_state_dict\"])\n",
    "else:\n",
    "    epoch_start = 1\n",
    "    losses = []\n",
    "\n",
    "\n",
    "if epoch_start > config.EPOCHS:\n",
    "    raise ValueError(\"Invalid number of epochs. Please choose a number greater than the number of epochs already trained.\")\n",
    "\n",
    "if config.SDE_SAMPLING_MODE not in config.SDE_SAMPLING_MODES:\n",
    "    raise ValueError('Invalid sde_sampling_mode. Please choose between \"euler_maruyama_only\" and \"langevin_mcmc_and_euler_maruyama\"')\n",
    "\n",
    "\n",
    "# MAIN TRAINING AND SAMPLING LOOP\n",
    "for epoch in range(epoch_start, config.EPOCHS + 1):\n",
    "    print(f\"EPOCH {epoch}/{config.EPOCHS}\")\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # TRAINING\n",
    "    for batch in tqdm.tqdm(data_loader, desc=\"Processing batches\"):\n",
    "        \n",
    "        # Load a batch of data and assign to device\n",
    "        x = batch[0].to(config.DEVICE)\n",
    "                \n",
    "        # Get random t for each sample in the batch\n",
    "        t = torch.rand(config.BATCH_SIZE, device=config.DEVICE) * (1.0 - config.EPSILON) + config.EPSILON\n",
    "        \n",
    "        # Calculate standard deviation\n",
    "        standard_deviation = model.marginal_probability_std(t)\n",
    "        \n",
    "        # Generate random z\n",
    "        z = torch.randn_like(x)\n",
    "        \n",
    "        # Generate noised image\n",
    "        x_noised = x + z * standard_deviation[:, None, None, None]\n",
    "        \n",
    "        # Calculate score\n",
    "        score = model(x_noised, t)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = torch.mean(torch.sum((score * standard_deviation[:, None, None, None] + z)**2, dim=(1,2,3)))\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "    # Collect losses for plotting\n",
    "    epoch_loss /= len(data_loader)\n",
    "    losses.append(epoch_loss)\n",
    "\n",
    "    # SAMPLING\n",
    "    if config.SAMPLING_TYPE == \"SDE\":\n",
    "        generated_images = model.sample_with_SDE(\n",
    "            sde_sampling_mode=config.SDE_SAMPLING_MODE,\n",
    "            signal_to_noise_ratio=config.SIGNAL_TO_NOISE_RATIO\n",
    "        )\n",
    "    \n",
    "    elif config.SAMPLING_TYPE == \"ODE\":        \n",
    "        generated_images = model.sample_with_ODE(\n",
    "            ode_error_tolerance=config.ODE_ERROR_TOLERANCE,\n",
    "            z = z\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError('Invalid sampling type. Please choose between \"SDE\" and \"ODE\"')\n",
    "    \n",
    "    # Display new images\n",
    "    model.display_images(generated_images)\n",
    "        \n",
    "    # Print mean loss at the end of epoch\n",
    "    print(f\"Epoch mean loss: {epoch_loss}\")\n",
    "\n",
    "    # Save progress (checkoint) every `config.CHECKPOINT_FREQUENCY` epochs\n",
    "    if epoch % config.CHECKPOINT_FREQUENCY == 0:\n",
    "            torch.save({\n",
    "                    \"epoch\": epoch,\n",
    "                    \"loss\": loss,\n",
    "                    \"losses\": losses,\n",
    "                    \"model_state_dict\": model.state_dict(),\n",
    "                    \"optimiser_state_dict\": optimiser.state_dict()\n",
    "                }, f\"score_matching_LATEST_{config.DATASET_NAME}_checkpoint_epoch_{epoch}.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
